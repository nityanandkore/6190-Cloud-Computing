# What are the problems with a “one size fits all” approach to relational databases?

The NoSQL movement, where “NoSQL” stands for “Not Only SQL” is based on the concept that relational databases are not the right database solution for all problems.  Relational databases are so ubiquitous in most organizations these days that many people may not even be aware that there are other types of databases, let alone when using another database might be preferable. Relational databases perform transaction update functions very well, particularly handling the difficult issues of consistency during update. Production strength relational databases can handle the complexity of two phase commit capability, where one business transaction affects multiple databases and tables, and all updates have to be effected at the same moment.
However, relational databases apply much of the same overhead required for complex update operations to every activity, and that can handicap them for other functions. Relational databases struggle with the efficiency of certain operations key to Big Data management. Firstly, they don’t scale well to very large sizes, and although grid solutions can help with this problem, the creation of new clusters on the grid is not dynamic and large data solutions become very expensive using relational databases. Secondly, they don’t do unstructured data search very well (i.e. google type searching) nor do they handle data in unexpected formats well. Thirdly, but not lastly, it is difficult to implement certain kinds of basic queries using SQL and relational databases, such as the shortest path between two points.

Social networking and Big Data organizations such as Facebook, Yahoo, Google, and Amazon were among the first to decide that relational databases were not good solutions for the volumes and types of data that they were dealing with, hence the development of the Hadoop file system, the MapReduce programming language, and associated databases such as Cassandra and HBase. One of the key capabilities of a Hadoop type environment is the ability to dynamically, or at least easily, expand the number of servers being used for data storage. The cost of storing large amounts of data in a relational database gets very expensive, where cost grows geometrically with the amount of data to be stored, reaching a limit in the petabyte range. The cost of storing data in a Hadoop solution grows linearly with the volume of data and there is no ultimate limit.

# How could a service like Google BigQuery change the way you deal with Data?

## Its Flexible Architecture Speeds Up Queries ##

Instead of operating across multiple compute clusters and requiring individual management of each, BigQuery distributes its computing resources dynamically, decreasing both the time it takes to scan through data and the cost of building a system. Instead of being stuck with rigid structures that are designed around multiple compute clusters, you can quickly distribute your computing power where it’s needed most. Moreover, BigQuery’s “serverless” build, a fully on-cloud design that prioritizes scalability and quickness in queries, means that you can easily scale and perform ad hoc analyses much faster than you would on cloud-based server structures. Even better, its decentralized design means it can perform these queries and derive insights from datasets that stretch into petabyte scale.

## It Offers a Scale-Friendly Pricing Structure ##

Tied to its unique serverless design, BigQuery offers a variety of pricing options that are friendlier to small businesses and those companies that experience evolving analytics needs. Instead of charging for server space and set prices for customized plans, BigQuery’s models are based on computing power and needs. The platform makes it significantly easier to configure and start running new instances without having to pay the cost of full servers.Alternatively, BigQuery’s pricing structure is based on computing demand and resource allocation. Users can set the resources they need on a per-second basis. More impressively, the platform also promises 100% resource utilization, meaning you only pay exactly for the computing power you’re using, in lieu of having to overpay based on fixed models. For small businesses, the model delivers the benefit of being highly elastic and scalable, so you can deploy the resources you need based on the task at hand without worrying about the cost of scaling back down.

## Access the Data You Need on Demand ##

When performing ad-hoc analysis and deriving real-time insights, using data that has been called and extracted from the warehouse can produce outdated results even in a matter of seconds. BigQuery solves this problem thanks to its cloud-native data warehousing model. This stems from two major tools that are designed to reduce friction: BigQuery’s batch ingests capability and its real-time ingest capacity. The former means that you can load thousands of data points easily into your analytics tools without having to incur a computational decrease. Instead of drawing from resources dedicated to analysis and management tools like SQL, batch ingest uses its own computing resources and does not impact real-time query abilities at all. Real-time ingest also makes it easy to load up to 100,000 rows of data per table for instant access, and in some cases even up to 1 million rows by deploying sharding. This makes real-time analysis significantly faster and more efficient.

## It Deploys AI to Optimize your Datasets ##

One of the more interesting, but exceptionally useful aspects of BigQuery is its ability to optimize your storage and datasets in the background. The platform utilizes Capacitor, a storage format, and system that uses AI to continuously evaluate data storage. Once it detects patterns, BigQuery will use them to optimize datasets into structures that are better suited to the types of analytics and queries users are performing regularly.Capacitor is fully transparent and automated, so it handles these processes on its own, but it always informs users of what is happening. The result is a platform that makes your queries faster as you make more queries thanks to machine learning algorithms. This also combines with BigQuery’s fully managed stack and its caching tools, which let users perform a free query if they’ve previously carried it out within the past 24 hours. All told, it reduces costs every 90 days, dropping storage prices nearly 50%.

## Choose the Right Tools for your BI ##

BigQuery can quickly optimize the way you query and compute your data, reducing your dependence on costly servers and fixed-price systems. However, while it’s certainly powerful, if you want to draw out truly useful BI insights or to build interactive dashboards, you’ll need to integrate Google BigQuery with additional tools like a BI platform.

# What problem does a “serverless” database like Athena solve?
# Post a screenshot of a lab where you had difficulty with a concept or learned something.
